# ğŸ§ª Test Flow â€” VerificaciÃ³n End-to-End de ShadowNet Defender

## Diagrama del flujo completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tu PC localâ”‚    â”‚  Cloudflare  â”‚    â”‚  Render (nube)  â”‚    â”‚  n8n Cloud   â”‚
â”‚             â”‚    â”‚  Tunnel      â”‚    â”‚                 â”‚    â”‚              â”‚
â”‚ Ollama:11434â”‚â—€â”€â”€â”€â”‚*.trycloudflareâ”‚â—€â”€â”€â”€â”‚ FastAPI /scan   â”‚â”€â”€â”€â–¶â”‚ Webhook      â”‚
â”‚             â”‚    â”‚  .com        â”‚    â”‚ /llm/explain    â”‚    â”‚ â†’ Email      â”‚
â”‚             â”‚    â”‚              â”‚    â”‚                 â”‚    â”‚ â†’ G.Drive    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Prerrequisitos

| Componente        | Estado esperado                            | CÃ³mo verificar                          |
| ----------------- | ------------------------------------------ | --------------------------------------- |
| Ollama            | Corriendo en localhost:11434               | `curl http://localhost:11434/v1/models` |
| Modelo descargado | llama3.2:3b (o el configurado)             | `ollama list`                           |
| Cloudflare Tunnel | Activo, mostrando URL pÃºblica              | Ver terminal de cloudflared             |
| Render            | Desplegado con OLLAMA_BASE_URL actualizada | Dashboard de Render â†’ Environment       |
| n8n               | Workflow activo con webhook                | Dashboard de n8n                        |

---

## Paso 1: Verificar Ollama local

```bash
# Â¿Ollama estÃ¡ corriendo?
ollama ps
# o
curl http://localhost:11434/v1/models

# Respuesta esperada:
# {"object":"list","data":[{"id":"llama3.2:3b","object":"model",...}]}
```

Si no estÃ¡ corriendo:

```bash
./fix-ollama.sh
```

---

## Paso 2: Verificar Cloudflare Tunnel

```bash
# En otra terminal:
cloudflared tunnel --url http://localhost:11434 --http-host-header="localhost:11434"

# Busca en la salida:
# https://random-string.trycloudflare.com
```

Verificar desde otra mÃ¡quina o navegador:

```bash
curl https://random-string.trycloudflare.com/v1/models
```

---

## Paso 3: Verificar que Render puede alcanzar Ollama

```bash
# Desde el dashboard de Render, verifica:
# OLLAMA_BASE_URL = https://random-string.trycloudflare.com/v1
# OLLAMA_MODEL = llama3.2:3b

# Health check del API:
curl https://shadownet-defender-extractor-v2.onrender.com/health
# Respuesta esperada: {"status":"ok","model":"loaded"}
```

---

## Paso 4: Test del endpoint /scan

```bash
curl "https://shadownet-defender-extractor-v2.onrender.com/scan?file_path=samples/procexp64.exe"
```

Respuesta esperada:

```json
{
  "file": "samples/procexp64.exe",
  "status": "detected",
  "score": 0.85,
  "label": "MALWARE",
  "confidence": "High",
  "scan_time_ms": 150.0,
  "details": { ... }
}
```

---

## Paso 5: Test del endpoint /llm/explain (Ollama vÃ­a Cloudflare Tunnel)

```bash
curl -X POST "https://shadownet-defender-extractor-v2.onrender.com/llm/explain" \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "ollama",
    "model": "llama3.2:3b",
    "scan_result": {
      "file": "samples/procexp64.exe",
      "status": "detected",
      "score": 0.91,
      "label": "MALWARE",
      "confidence": "High",
      "details": {
        "entropy": 6.8,
        "suspicious_imports": ["VirtualAlloc", "WriteProcessMemory"],
        "suspicious_sections": [".rsrc"],
        "top_features": [
          {"name": "byte_entropy_mean", "value": 6.8, "impact": 0.15}
        ]
      }
    }
  }'
```

Respuesta esperada:

```json
{
  "ok": true,
  "scan_result": { ... },
  "llm": {
    "provider": "ollama",
    "model": "llama3.2:3b",
    "response_text": "{\"resumen_ejecutivo\":\"...\",\"explicacion_tecnica\":\"...\"}",
    "prompt_version": "v1"
  }
}
```

**Verificaciones clave:**

- âœ… `llm.provider` = `"ollama"` (no otro motor)
- âœ… `llm.model` = `"llama3.2:3b"` (el modelo configurado)
- âœ… `llm.response_text` contiene JSON vÃ¡lido con la explicaciÃ³n
- âœ… En la terminal de cloudflared, deberÃ­as ver requests entrantes

---

## Paso 6: Verificar n8n webhook

DespuÃ©s de llamar a `/scan`, verifica en n8n:

1. Abre el dashboard de n8n: https://ivainx21.app.n8n.cloud
2. Ve al workflow de ShadowNet
3. Verifica que el webhook recibiÃ³ el payload
4. Verifica que el email fue enviado
5. Verifica que el reporte se guardÃ³ en Google Drive

```bash
# Test manual del webhook:
curl -X POST "https://shadownet-defender-extractor-v2.onrender.com/automation/test"
```

---

## Paso 7: Test local completo (sin Render)

Para probar todo localmente sin Render:

```bash
# Terminal 1: Ollama
ollama serve

# Terminal 2: FastAPI local
export OLLAMA_BASE_URL="http://localhost:11434/v1"
export OLLAMA_MODEL="llama3.2:3b"
python -m uvicorn api_server:app --host 127.0.0.1 --port 8000

# Terminal 3: Tests
curl http://localhost:8000/health
curl "http://localhost:8000/scan?file_path=samples/procexp64.exe"
curl -X POST "http://localhost:8000/llm/explain" \
  -H "Content-Type: application/json" \
  -d '{"file_path": "samples/procexp64.exe"}'
```

---

## Paso 8: Tests unitarios

```bash
# Ejecutar todos los tests (no requieren Ollama corriendo)
python -m pytest tests/ -v

# Solo tests del cliente Ollama
python -m pytest tests/test_ollama_client.py -v

# Solo tests de integraciÃ³n CLI + LLM
python -m pytest tests/test_cli_llm_integration.py -v
```

---

## Checklist de verificaciÃ³n final

| #   | VerificaciÃ³n                                      | âœ…/âŒ |
| --- | ------------------------------------------------- | ----- |
| 1   | Ollama corriendo localmente                       |       |
| 2   | Modelo descargado (`ollama list`)                 |       |
| 3   | Cloudflare Tunnel activo (URL pÃºblica visible)    |       |
| 4   | URL pÃºblica responde (`curl .../v1/models`)       |       |
| 5   | OLLAMA_BASE_URL configurada en Render             |       |
| 6   | `/health` responde OK                             |       |
| 7   | `/scan` retorna resultado ML                      |       |
| 8   | `/llm/explain` retorna explicaciÃ³n de Ollama      |       |
| 9   | n8n webhook recibe payload                        |       |
| 10  | Email enviado por n8n                             |       |
| 11  | Reporte guardado en Google Drive                  |       |
| 12  | Modelo en respuesta LLM coincide con OLLAMA_MODEL |       |
| 13  | Tests unitarios pasan (`pytest tests/ -v`)        |       |

---

## Troubleshooting

### `/llm/explain` retorna error de conexiÃ³n

```
"error": "No se pudo conectar a Ollama en https://xxxx.trycloudflare.com/v1"
```

**Causa:** El Cloudflare Tunnel se cerrÃ³ o la URL cambiÃ³.
**SoluciÃ³n:** Reinicia cloudflared y actualiza OLLAMA_BASE_URL en Render.

### `/llm/explain` retorna timeout

```
"error": "Ollama no respondiÃ³ en 120 segundos"
```

**Causa:** El modelo es muy grande o es la primera carga.
**SoluciÃ³n:** Espera a que Ollama cargue el modelo, o usa uno mÃ¡s ligero (`phi3:mini`).

### n8n no recibe el webhook

**Causa:** El workflow no estÃ¡ activo o el webhook URL cambiÃ³.
**SoluciÃ³n:** Activa el workflow en n8n y verifica las URLs en las variables de entorno.

### Cloudflared muestra "Bad Request"

**Causa:** Falta `--http-host-header="localhost:11434"`.
**SoluciÃ³n:** Reinicia cloudflared con el flag correcto.
