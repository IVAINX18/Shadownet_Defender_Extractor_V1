# ğŸ¬ GuiÃ³n de DemostraciÃ³n â€” ShadowNet Defender

## Reporte de avances para presentaciÃ³n universitaria

> **Enfoque de hoy:** DemostraciÃ³n local de detecciÃ³n ML + explicaciÃ³n LLM con Ollama.
> Render y n8n se mencionan como arquitectura futura pero no se demuestran en profundidad.

---

## ğŸ“‹ Orden de la demostraciÃ³n

| #   | QuÃ© mostrar                                    | Tiempo aprox. | Archivo/Comando                  |
| --- | ---------------------------------------------- | ------------- | -------------------------------- |
| 1   | Verificar Ollama corriendo + modelo descargado | 2 min         | `fix-ollama.sh`                  |
| 2   | MÃ©tricas del modelo ML (precisiÃ³n, recall, F1) | 3 min         | `evaluate_model_metrics.py`      |
| 3   | Pruebas de robustez adversarial                | 3 min         | `test_robustness.py`             |
| 4   | Explicabilidad global (feature importance)     | 2 min         | `explain_global_model.py`        |
| 5   | Escaneo de archivo real (PE) por CLI           | 2 min         | `cli.py scan`                    |
| 6   | ExplicaciÃ³n LLM con Ollama local               | 3 min         | `cli.py scan --explain`          |
| 7   | API local + endpoint /llm/explain              | 3 min         | `api_server.py` + `curl`         |
| 8   | Tests unitarios automatizados                  | 2 min         | `pytest`                         |
| 9   | VerificaciÃ³n de readiness completa             | 2 min         | `verify_readiness.py`            |
| 10  | MenciÃ³n: Render + n8n + Cloudflare Tunnel      | 2 min         | Mostrar `render.yaml` y diagrama |

---

## ğŸ”§ PreparaciÃ³n previa (antes de la demo)

Ejecuta esto **antes** de la presentaciÃ³n para que todo estÃ© listo:

```bash
# 1. Instalar dependencias del proyecto
pip install -r requirements.txt

# 2. Verificar/instalar Ollama + descargar modelo
chmod +x fix-ollama.sh
./fix-ollama.sh

# 3. Verificar que Ollama responde
curl http://localhost:11434/v1/models

# 4. Configurar variables de entorno para uso local
export OLLAMA_BASE_URL="http://localhost:11434/v1"
export OLLAMA_MODEL="llama3.2:3b"
```

---

## ğŸ¯ DEMO 1 â€” Ollama funcionando (2 min)

**QuÃ© demuestras:** Que el LLM estÃ¡ corriendo localmente.

```bash
# Verificar que Ollama estÃ¡ activo
ollama list

# Ver modelos cargados en memoria
ollama ps

# Test rÃ¡pido del endpoint
curl http://localhost:11434/v1/models
```

**QuÃ© decir:** _"Ollama es un servidor de modelos de lenguaje que corre localmente. AquÃ­ vemos que tenemos el modelo llama3.2:3b descargado y listo. Este modelo se usa para generar explicaciones tÃ©cnicas de los resultados de detecciÃ³n."_

---

## ğŸ¯ DEMO 2 â€” MÃ©tricas del modelo ML (3 min)

**Archivo:** [`evaluate_model_metrics.py`](../evaluate_model_metrics.py)

```bash
python evaluate_model_metrics.py
```

**Salida esperada:**

```
=== EVALUACIÃ“N EXPERIMENTAL DEL MODELO ===
Datos Cargados: (1000, 2381) muestras.
------------------------------------------------
MÃ‰TRICAS PRINCIPALES
------------------------------------------------
Accuracy:  0.9XXX
Precision: 0.9XXX
Recall:    0.9XXX
F1-Score:  0.9XXX
ROC-AUC:   0.9XXX
------------------------------------------------
MATRIZ DE CONFUSIÃ“N
------------------------------------------------
TN (Benignos detectados): XXX
FP (Falsas Alarmas):      XX
FN (Malware No Detectado): XX
TP (Malware Detectado):   XXX
```

**QuÃ© decir:** _"El modelo fue entrenado con el dataset SOREL-20M de 20 millones de muestras reales. AquÃ­ vemos las mÃ©tricas de evaluaciÃ³n: Accuracy, Precision, Recall, F1-Score y ROC-AUC. El Recall es especialmente importante porque mide cuÃ¡nto malware detectamos â€” un Recall bajo significa que estamos dejando pasar amenazas."_

---

## ğŸ¯ DEMO 3 â€” Pruebas de robustez adversarial (3 min)

**Archivo:** [`test_robustness.py`](../test_robustness.py)

```bash
python test_robustness.py
```

**QuÃ© decir:** _"Estas pruebas simulan tÃ©cnicas de evasiÃ³n que usan los atacantes reales: inyecciÃ³n de bytes para cambiar la entropÃ­a, perturbaciÃ³n de imports, y ruido gaussiano. Verificamos que el modelo mantiene su capacidad de detecciÃ³n incluso bajo estas condiciones adversariales."_

---

## ğŸ¯ DEMO 4 â€” Explicabilidad global del modelo (2 min)

**Archivo:** [`explain_global_model.py`](../explain_global_model.py)

```bash
python explain_global_model.py
```

**QuÃ© decir:** _"Usamos Permutation Importance para entender quÃ© bloques de caracterÃ­sticas son mÃ¡s relevantes para la decisiÃ³n del modelo. Esto es clave para la explicabilidad (XAI) â€” no solo detectamos malware, sino que podemos explicar por quÃ©."_

---

## ğŸ¯ DEMO 5 â€” Escaneo de archivo real por CLI (2 min)

**Archivo:** [`cli.py`](../cli.py)

```bash
# Escaneo simple (solo ML, sin LLM)
python cli.py scan samples/procexp64.exe
```

**Salida esperada:** JSON con `score`, `label`, `confidence`, `details`.

**QuÃ© decir:** _"AquÃ­ escaneamos un ejecutable real (Process Explorer de Microsoft). El motor ML extrae 2,381 caracterÃ­sticas del archivo PE, las normaliza con el scaler, y las pasa por el modelo ONNX para obtener un score de maliciosidad."_

---

## ğŸ¯ DEMO 6 â€” ExplicaciÃ³n LLM con Ollama (3 min) â­ DEMO PRINCIPAL

**Archivo:** [`cli.py`](../cli.py) + [`core/llm/ollama_client.py`](../core/llm/ollama_client.py)

```bash
# Escaneo + explicaciÃ³n LLM (requiere Ollama corriendo)
export OLLAMA_BASE_URL="http://localhost:11434/v1"
export OLLAMA_MODEL="llama3.2:3b"

python cli.py scan samples/procexp64.exe --explain --provider ollama --model llama3.2:3b
```

**Alternativa con scan simulado (si el escaneo real tarda):**

```bash
python cli.py llm-explain --scan-json samples/malware_simulated_scan.json --provider ollama --model llama3.2:3b
```

**Salida esperada:** JSON con `scan_result` + bloque `llm` con `response_text` conteniendo la explicaciÃ³n tÃ©cnica.

**QuÃ© decir:** _"AquÃ­ es donde entra la IA generativa. DespuÃ©s de que el modelo ML clasifica el archivo, enviamos el resultado a Ollama (un LLM corriendo localmente) que genera una explicaciÃ³n tÃ©cnica estructurada. El LLM NO detecta malware â€” solo explica el resultado del modelo ML. Esto es clave: separamos la detecciÃ³n (ML) de la explicaciÃ³n (LLM)."_

---

## ğŸ¯ DEMO 7 â€” API local + endpoint /llm/explain (3 min)

**Archivo:** [`api_server.py`](../api_server.py)

**Terminal 1 â€” Levantar API:**

```bash
export OLLAMA_BASE_URL="http://localhost:11434/v1"
export OLLAMA_MODEL="llama3.2:3b"
python -m uvicorn api_server:app --host 127.0.0.1 --port 8000
```

**Terminal 2 â€” Probar endpoints:**

```bash
# Health check
curl http://localhost:8000/health

# ExplicaciÃ³n LLM vÃ­a API
curl -X POST "http://localhost:8000/llm/explain" \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "ollama",
    "model": "llama3.2:3b",
    "scan_result": {
      "label": "MALWARE",
      "score": 0.97,
      "confidence": "High",
      "details": {
        "entropy": 7.89,
        "suspicious_imports": ["VirtualAlloc", "WriteProcessMemory", "CreateRemoteThread"],
        "suspicious_sections": [".rwx", ".packed"],
        "top_features": [
          {"name": "imports_hash_10", "value": 0.91, "impact": "high"},
          {"name": "section_entropy_high", "value": 0.87, "impact": "high"}
        ]
      }
    }
  }'
```

**QuÃ© decir:** _"La API FastAPI expone los mismos endpoints que usarÃ¡ Render en producciÃ³n. El endpoint /llm/explain recibe un resultado de escaneo y devuelve la explicaciÃ³n del LLM. En producciÃ³n, esta API corre en Render y se conecta a Ollama vÃ­a Cloudflare Tunnel."_

---

## ğŸ¯ DEMO 8 â€” Tests unitarios (2 min)

**Archivos:** [`tests/test_ollama_client.py`](../tests/test_ollama_client.py), [`tests/test_llm_prompt_builder.py`](../tests/test_llm_prompt_builder.py)

```bash
python -m pytest tests/test_ollama_client.py tests/test_llm_prompt_builder.py -v
```

**Salida esperada:**

```
tests/test_ollama_client.py::test_ollama_client_generate_success PASSED
tests/test_ollama_client.py::test_ollama_client_generate_with_custom_model PASSED
tests/test_ollama_client.py::test_ollama_client_connection_error PASSED
tests/test_ollama_client.py::test_ollama_client_timeout_error PASSED
tests/test_ollama_client.py::test_ollama_client_empty_response PASSED
tests/test_ollama_client.py::test_ollama_client_prod_localhost_raises PASSED
tests/test_llm_prompt_builder.py::test_extract_scan_summary_only_allowed_fields PASSED
tests/test_llm_prompt_builder.py::test_build_llm_prompt_contains_guardrails_and_summary PASSED

8 passed
```

**QuÃ© decir:** _"Tenemos tests unitarios que verifican el cliente Ollama, el constructor de prompts, y el manejo de errores. Estos tests usan mocks â€” no necesitan Ollama corriendo. Verificamos: respuestas exitosas, modelos custom, errores de conexiÃ³n, timeouts, respuestas vacÃ­as, y validaciÃ³n de entorno de producciÃ³n."_

---

## ğŸ¯ DEMO 9 â€” VerificaciÃ³n de readiness (2 min)

**Archivo:** [`verify_readiness.py`](../verify_readiness.py)

```bash
python verify_readiness.py
```

**QuÃ© decir:** _"Este script ejecuta un flujo completo simulado: inferencia del modelo, generaciÃ³n de prompts, y configuraciÃ³n del servicio de explicaciÃ³n. Verifica que todos los componentes estÃ¡n correctamente integrados."_

---

## ğŸ¯ DEMO 10 â€” MenciÃ³n de arquitectura cloud (2 min)

**No ejecutar, solo mostrar archivos:**

```bash
# Mostrar configuraciÃ³n de Render
cat render.yaml

# Mostrar diagrama de arquitectura
cat docs/cloudflare-tunnel-setup.md
```

**QuÃ© decir:** _"Para producciÃ³n, el backend FastAPI se despliega en Render (nube gratuita). Ollama sigue corriendo localmente pero se expone pÃºblicamente con Cloudflare Tunnel, que crea una URL HTTPS segura. Render consume esa URL para las explicaciones LLM. AdemÃ¡s, tenemos n8n como orquestador de automatizaciÃ³n que envÃ­a emails y guarda reportes en Google Drive. Esto lo demostraremos en detalle en la prÃ³xima presentaciÃ³n."_

Mostrar este diagrama:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTPS      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Render (API) â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Cloudflare Tunnel  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Ollama :11434â”‚
â”‚ (nube)       â”‚                â”‚  *.trycloudflare.comâ”‚               â”‚ (PC local)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ webhook
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  n8n Cloud   â”‚ â†’ Email + Google Drive + AuditorÃ­a
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Resumen de archivos clave para la demo

| Archivo                               | PropÃ³sito                                         |
| ------------------------------------- | ------------------------------------------------- |
| `fix-ollama.sh`                       | Script para instalar/verificar/iniciar Ollama     |
| `evaluate_model_metrics.py`           | MÃ©tricas ML: Accuracy, Precision, Recall, F1, AUC |
| `test_robustness.py`                  | Pruebas adversariales de robustez                 |
| `explain_global_model.py`             | Explicabilidad global (feature importance)        |
| `cli.py`                              | CLI unificada (scan, llm-explain, verify-model)   |
| `api_server.py`                       | API FastAPI con endpoints /scan, /llm/explain     |
| `core/llm/ollama_client.py`           | Cliente Ollama (OpenAI SDK)                       |
| `core/llm/prompt_builder.py`          | Constructor de prompts seguros                    |
| `llm_agent_bridge.py`                 | Puente/adaptador LLM                              |
| `verify_readiness.py`                 | VerificaciÃ³n de integraciÃ³n completa              |
| `tests/test_ollama_client.py`         | Tests unitarios del cliente Ollama                |
| `tests/test_llm_prompt_builder.py`    | Tests del constructor de prompts                  |
| `render.yaml`                         | ConfiguraciÃ³n de deploy en Render                 |
| `samples/malware_simulated_scan.json` | Resultado de escaneo simulado para demos          |

---

## âš¡ Comandos rÃ¡pidos (cheat sheet)

```bash
# PreparaciÃ³n
pip install -r requirements.txt
./fix-ollama.sh
export OLLAMA_BASE_URL="http://localhost:11434/v1"
export OLLAMA_MODEL="llama3.2:3b"

# MÃ©tricas ML
python evaluate_model_metrics.py
python test_robustness.py
python explain_global_model.py

# Escaneo + LLM
python cli.py scan samples/procexp64.exe
python cli.py scan samples/procexp64.exe --explain --provider ollama --model llama3.2:3b
python cli.py llm-explain --scan-json samples/malware_simulated_scan.json --provider ollama

# API
python -m uvicorn api_server:app --host 127.0.0.1 --port 8000
curl http://localhost:8000/health
curl -X POST http://localhost:8000/llm/explain -H "Content-Type: application/json" -d '{"scan_result":{"label":"MALWARE","score":0.97,"confidence":"High","details":{}}}'

# â”€â”€ DEMO: API en Render (producciÃ³n) â”€â”€
curl https://shadownet-defender-extractor-v2.onrender.com/health

# Tests
python -m pytest tests/test_ollama_client.py tests/test_llm_prompt_builder.py -v

# Readiness
python verify_readiness.py
```
